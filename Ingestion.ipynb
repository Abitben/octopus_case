{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical problem : it seems that you can't upload from UI interface the .db file to Amazon Redshift.\n",
    "\n",
    "Solution : We can transform .db file to .parquet and ingest the data to S3 bucket then Redshift   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload raw file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.35.47-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from boto3) (0.10.0)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.36.0,>=1.35.47\n",
      "  Downloading botocore-1.35.47-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.47->boto3) (1.26.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.47->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.47->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.35.47 botocore-1.35.47 s3transfer-0.10.3\n"
     ]
    }
   ],
   "source": [
    "# install the boto3 library for AWS\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# library for parsing .env files and protecting sensitive information\n",
    "# don't forget to create a .env file with your AWS credentials and reference it in the .gitignore if the repository is public\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "access_key = os.getenv(\"access_key_S3\")\n",
    "secret_key  = os.getenv(\"secret_key_S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.s3.transfer import S3Transfer\n",
    "import boto3\n",
    "\n",
    "# Upload the database to the S3 bucket\n",
    "filepath = \"calls_case_study.db\"\n",
    "bucket_name = \"octopus-energy-ops\"\n",
    "folder_name = \"raw_data\"\n",
    "filename = filepath\n",
    "\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=access_key,aws_secret_access_key=secret_key)\n",
    "transfer = S3Transfer(client)\n",
    "transfer.upload_file(filepath, bucket_name, folder_name+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from S3 and prepare data to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to import sqlite (.db) to Python\n",
    "https://stackoverflow.com/questions/62340498/open-database-files-db-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "# Specify the S3 bucket and file path\n",
    "bucket_name = 'octopus-energy-ops'\n",
    "file_path = 'raw_data/calls_case_study.db'\n",
    "\n",
    "# create a new folder in the current directory\n",
    "os.makedirs('raw_data', exist_ok=True)\n",
    "\n",
    "s3_client.download_file(bucket_name, file_path, 'raw_data/calls_case_study.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name : [('call_reason',), ('account',), ('call',)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "try:\n",
    "    conn = sqlite3.connect(\"raw_data/calls_case_study.db\")    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Create a cursor to retrieve tables from the database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Table Name : {cursor.fetchall()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Reason Shape : 3\n",
      "Number of Rows in Reason Table : [(145,)]\n",
      "Table Reason Info : [(0, 'id', 'INTEGER', 0, None, 0), (1, 'reason', 'TEXT', 0, None, 0), (2, 'category', 'TEXT', 0, None, 0)]\n",
      "***************\n",
      "Table Account Shape : 3\n",
      "Number of Rows in Account Table :  [(75815,)]\n",
      "Table Account Info : [(0, 'id', 'INTEGER', 0, None, 0), (1, 'reason', 'TEXT', 0, None, 0), (2, 'category', 'TEXT', 0, None, 0)]\n",
      "***************\n",
      "Table Call Shape : 7\n",
      "Number of Rows in Call Table :  [(100000,)]\n",
      "Table Call Info : [(0, 'id', 'TEXT', 0, None, 0), (1, 'called_at', 'TEXT', 0, None, 0), (2, 'agent_id', 'TEXT', 0, None, 0), (3, 'reason_id', 'TEXT', 0, None, 0), (4, 'talk_time', 'REAL', 0, None, 0), (5, 'direction', 'TEXT', 0, None, 0), (6, 'account_id', 'TEXT', 0, None, 0)]\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "# retrieve shape from the .db file\n",
    "\n",
    "reason_columns = cursor.execute(\"PRAGMA table_info(call_reason);\").fetchall()\n",
    "reason_rows = cursor.execute(\"SELECT count(*) FROM call_reason;\").fetchall()\n",
    "print(f\"Table Reason Shape : {len(reason_columns)}\")\n",
    "print(f'Number of Rows in Reason Table : {reason_rows}')\n",
    "print(f\"Table Reason Info : {reason_columns}\")\n",
    "print(\"***************\")\n",
    "\n",
    "account_columns = cursor.execute(\"PRAGMA table_info(account);\").fetchall()\n",
    "account_rows = cursor.execute(\"SELECT count(*) FROM account;\").fetchall()\n",
    "print(f\"Table Account Shape : {len(reason_columns)}\")\n",
    "print('Number of Rows in Account Table : ', account_rows)\n",
    "print(f\"Table Account Info : {reason_columns}\")\n",
    "print(\"***************\")\n",
    "\n",
    "call_columns = cursor.execute(\"PRAGMA table_info(call);\").fetchall()\n",
    "call_rows = cursor.execute(\"SELECT count(*) FROM call;\").fetchall()\n",
    "print(f\"Table Call Shape : {len(call_columns)}\")\n",
    "print('Number of Rows in Call Table : ', call_rows)\n",
    "print(f\"Table Call Info : {call_columns}\")\n",
    "print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_callreason\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145 entries, 0 to 144\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        145 non-null    int64 \n",
      " 1   reason    145 non-null    object\n",
      " 2   category  145 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.5+ KB\n",
      "None\n",
      "********************************\n",
      "df_account\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75815 entries, 0 to 75814\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             75815 non-null  int64 \n",
      " 1   sales_channel  75815 non-null  object\n",
      " 2   sign_up_date   75815 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "********************************\n",
      "df_call\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   id          100000 non-null  object \n",
      " 1   called_at   100000 non-null  object \n",
      " 2   agent_id    100000 non-null  object \n",
      " 3   reason_id   100000 non-null  object \n",
      " 4   talk_time   100000 non-null  float64\n",
      " 5   direction   100000 non-null  object \n",
      " 6   account_id  100000 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 5.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the tables into pandas dataframes and confirm there is no missing data\n",
    "\n",
    "df_callreason = pd.read_sql_query('SELECT * FROM call_reason', conn)\n",
    "df_account = pd.read_sql_query('SELECT * FROM account', conn)\n",
    "df_call = pd.read_sql_query('SELECT * FROM call', conn)\n",
    "\n",
    "print(\"df_callreason\")\n",
    "print(df_callreason.info())\n",
    "print(\"********************************\")\n",
    "print(\"df_account\")\n",
    "print(df_account.info())\n",
    "print(\"********************************\")\n",
    "print(\"df_call\")\n",
    "print(df_call.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the parquet files\n",
    "os.makedirs('prep_data', exist_ok=True)\n",
    "\n",
    "df_callreason.to_parquet('prep_data/call_reason.parquet')\n",
    "df_account.to_parquet('prep_data/account.parquet')\n",
    "df_call.to_parquet('prep_data/call.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file processed : account.parquet\n",
      "account.parquet uploaded to S3 in the folder prep_data\n",
      "***************\n",
      "Current file processed : call_reason.parquet\n",
      "call_reason.parquet uploaded to S3 in the folder prep_data\n",
      "***************\n",
      "Current file processed : call.parquet\n",
      "call.parquet uploaded to S3 in the folder prep_data\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "from boto3.s3.transfer import S3Transfer\n",
    "import boto3\n",
    "\n",
    "for file in os.listdir('prep_data'):\n",
    "    print(f'Current file processed : {file}')\n",
    "    filepath = 'prep_data/' + file\n",
    "    bucket_name = 'octopus-energy-ops'\n",
    "    folder_name = 'prep_data'\n",
    "    filename = file\n",
    "    client = boto3.client('s3', aws_access_key_id=access_key,aws_secret_access_key=secret_key)\n",
    "    transfer = S3Transfer(client)\n",
    "    transfer.upload_file(filepath, bucket_name, folder_name+\"/\"+filename)\n",
    "    print(f'{file} uploaded to S3 in the folder {folder_name}')\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to map pandas dtypes to redshift dtypes\n",
    "\n",
    "dtype_to_redshift = {\n",
    "    \"int64\": \"BIGINT\",\n",
    "    \"float64\": \"DOUBLE PRECISION\",\n",
    "    \"bool\": \"BOOLEAN\",\n",
    "    \"datetime64[ns]\": \"TIMESTAMP\",\n",
    "    \"timedelta[ns]\": \"INTERVAL\",\n",
    "    \"object\": \"VARCHAR(MAX)\",\n",
    "    \"category\": \"VARCHAR(MAX)\",\n",
    "    \"int32\": \"INTEGER\",\n",
    "    \"float32\": \"REAL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://octopus-energy-ops/prep_data/account.parquet\n",
      "s3://octopus-energy-ops/prep_data/call.parquet\n",
      "s3://octopus-energy-ops/prep_data/call_reason.parquet\n",
      "['s3://octopus-energy-ops/prep_data/account.parquet', 's3://octopus-energy-ops/prep_data/call.parquet', 's3://octopus-energy-ops/prep_data/call_reason.parquet']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "access_key = os.getenv(\"access_key_S3\")\n",
    "secret_key  = os.getenv(\"secret_key_S3\")\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "# Specify the bucket name\n",
    "bucket_name = 'octopus-energy-ops'\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "folder_name = 'prep_data'\n",
    "\n",
    "# List objects in the bucket to prepare COPY command in Redshift\n",
    "aws_files = []\n",
    "for obj in response['Contents']:\n",
    "  if obj['Key'].startswith(folder_name):\n",
    "    aws_uri = f\"s3://{bucket_name}/{obj['Key']}\"\n",
    "    print(aws_uri)\n",
    "    aws_files.append(aws_uri)\n",
    "\n",
    "print(aws_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redshift_connector\n",
      "  Downloading redshift_connector-2.1.3-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.0/130.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (4.11.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (2023.3)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (4.9.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (1.35.47)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (68.0.0)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (1.35.47)\n",
      "Requirement already satisfied: packaging in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from redshift_connector) (23.1)\n",
      "Collecting scramp<1.5.0,>=1.2.0\n",
      "  Downloading scramp-1.4.5-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector) (2.3.2.post1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (0.10.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (3.4)\n",
      "Collecting asn1crypto>=1.5.1\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/benjamindupaquier/anaconda3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift_connector) (1.16.0)\n",
      "Installing collected packages: asn1crypto, scramp, redshift_connector\n",
      "Successfully installed asn1crypto-1.5.1 redshift_connector-2.1.3 scramp-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'table_name': 'account', 'columns': [{'name': 'id', 'type': 'BIGINT'}, {'name': 'sales_channel', 'type': 'VARCHAR(MAX)'}, {'name': 'sign_up_date', 'type': 'VARCHAR(MAX)'}]}, {'table_name': 'call_reason', 'columns': [{'name': 'id', 'type': 'BIGINT'}, {'name': 'reason', 'type': 'VARCHAR(MAX)'}, {'name': 'category', 'type': 'VARCHAR(MAX)'}]}, {'table_name': 'call', 'columns': [{'name': 'id', 'type': 'VARCHAR(MAX)'}, {'name': 'called_at', 'type': 'VARCHAR(MAX)'}, {'name': 'agent_id', 'type': 'VARCHAR(MAX)'}, {'name': 'reason_id', 'type': 'VARCHAR(MAX)'}, {'name': 'talk_time', 'type': 'DOUBLE PRECISION'}, {'name': 'direction', 'type': 'VARCHAR(MAX)'}, {'name': 'account_id', 'type': 'VARCHAR(MAX)'}]}]\n"
     ]
    }
   ],
   "source": [
    "# create a mapping between parquet dtypes and redshift dtypes\n",
    "list_types = []\n",
    "\n",
    "# loop through the files in the prep_data folder\n",
    "for file in os.listdir('prep_data'):\n",
    "    table_dic =  {\n",
    "        'table_name': file.split('.')[0],\n",
    "        'columns': []\n",
    "    }\n",
    "    # read the parquet file with pandas\n",
    "    df = pd.read_parquet(f'prep_data/{file}')\n",
    "    for col in df.columns:\n",
    "        table_dic['columns'].append({\n",
    "            'name': col,\n",
    "            # if we filter on a specific series, we can get the dtype with .dtype\n",
    "            'type': dtype_to_redshift[str(df[col].dtype)]\n",
    "        })\n",
    "    list_types.append(table_dic)\n",
    "\n",
    "print(list_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table_name': 'account', 'columns': [{'name': 'id', 'type': 'BIGINT'}, {'name': 'sales_channel', 'type': 'VARCHAR(MAX)'}, {'name': 'sign_up_date', 'type': 'VARCHAR(MAX)'}]}\n",
      "id BIGINT, sales_channel VARCHAR(MAX), sign_up_date VARCHAR(MAX)\n",
      "Table account created\n",
      "{'table_name': 'call_reason', 'columns': [{'name': 'id', 'type': 'BIGINT'}, {'name': 'reason', 'type': 'VARCHAR(MAX)'}, {'name': 'category', 'type': 'VARCHAR(MAX)'}]}\n",
      "id BIGINT, reason VARCHAR(MAX), category VARCHAR(MAX)\n",
      "Table call_reason created\n",
      "{'table_name': 'call', 'columns': [{'name': 'id', 'type': 'VARCHAR(MAX)'}, {'name': 'called_at', 'type': 'VARCHAR(MAX)'}, {'name': 'agent_id', 'type': 'VARCHAR(MAX)'}, {'name': 'reason_id', 'type': 'VARCHAR(MAX)'}, {'name': 'talk_time', 'type': 'DOUBLE PRECISION'}, {'name': 'direction', 'type': 'VARCHAR(MAX)'}, {'name': 'account_id', 'type': 'VARCHAR(MAX)'}]}\n",
      "id VARCHAR(MAX), called_at VARCHAR(MAX), agent_id VARCHAR(MAX), reason_id VARCHAR(MAX), talk_time DOUBLE PRECISION, direction VARCHAR(MAX), account_id VARCHAR(MAX)\n",
      "Table call created\n",
      "Table account created\n",
      "Table call created\n",
      "Table call_reason created\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Retrieve the Redshift credentials from the .env file\n",
    "load_dotenv()\n",
    "redshift_user = os.getenv(\"redshift_user\")\n",
    "redshift_password = os.getenv(\"redshift_password\")\n",
    "iam_role = os.getenv(\"iam_role\")\n",
    "\n",
    "# Connect to Redshift\n",
    "conn = psycopg2.connect(\n",
    "  host='octopus-energy-ops.202533530775.eu-west-3.redshift-serverless.amazonaws.com',\n",
    "  port=5439,\n",
    "  database='dev',\n",
    "  user=redshift_user,\n",
    "  password=redshift_password\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables from the list_types\n",
    "for table in list_types:\n",
    "  table_name = table['table_name']\n",
    "  columns = table['columns']\n",
    "  # create a string with the columns and their types\n",
    "  columns_str = ', '.join([f\"{col['name']} {col['type']}\" for col in columns])\n",
    "  cursor.execute(f\"CREATE TABLE IF NOT EXISTS dev.raw_data.{table_name} ({columns_str})\")\n",
    "  print(f\"Table {table_name} created\")\n",
    "  conn.commit()\n",
    "\n",
    "# Once tables schema are created, we can execute the COPY command for each file of the S3 bucket to populate the tables\n",
    "for file in aws_files:\n",
    "  table_name = file.split('/')[-1].split('.')[0]\n",
    "  cursor.execute(f\"COPY dev.raw_data.{table_name} FROM '{file}' IAM_ROLE '{iam_role}' FORMAT AS PARQUET\")\n",
    "  print(f\"Table {table_name} populated\")\n",
    "  conn.commit()\n",
    "\n",
    "# # Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
